\documentclass[10pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, top=1in, bottom=1.25in, left=0.75in, right=0.75in]{geometry}

\begin{document}
\pagestyle{fancy}
\fancyhead[R]{Robert Sch√ºle 319782, Christoph Ende 331655}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.1 Binomial Coefficients}

\begin{eqnarray}C_{(p,N)}\;+\;C_{(p,N-1)}\;&=&\;C_{(p+1,N)} \\
2 \sum_{k=0}^{N-1} {p-1 \choose k} \;+\; 2 \sum_{k=1}^{N-1} {p-1 \choose k-1} \;&=&\; 2 \sum_{k=0}^{N-1} {p \choose k} \;\;\;\Big{|}\mbox{\small{extend k range in $2^{nd}$ sum:}} {n \choose -1} = 0\; \forall\; n \in \mathbb{N} \\
2 \sum_{k=0}^{N-1} {p-1 \choose k} \;+\; 2 \sum_{k=0}^{N-1} {p-1 \choose k-1}  \;&=&\; 2 \sum_{k=0}^{N-1} {p \choose k} \\
2 \sum_{k=0}^{N-1}\Big{[} {p-1 \choose k} \;+\; {p-1 \choose k-1} \Big{]} \;&=&\; 2 \sum_{k=0}^{N-1} {p \choose k} \;\;\;\Big{|} \; \mbox{\small{apply (2) from task sheet}} \\
2 \sum_{k=0}^{N-1} {p \choose k} \;&=&\; 2 \sum_{k=0}^{N-1} {p \choose k}_{\tiny{\qed}}
\end{eqnarray}
\noindent

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.2 Geometry of Linear Classification}
\begin{enumerate}[a)]
\item
The decision boundary is a hyperplane defined by $(w^Tx + b = 0)$. $w$ decides the orientation,
$b$ decides the offset from the origin. $w$ points in the direction of positive classification values $y(x)$. (Figure \ref{820})
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{820.png}
    \caption{}
	\label{820}
\end{figure}
\begin{itemize}
\item the decision boundary is given by $\{x\;|\;w^Tx + b=0\}$
\item in the 2-dimensional case this can be expressed as: $x_1 = \;-\frac{w_0}{w_1}x_0 \;-\; \frac{b}{w_1}$
\item as one can see, the slope of the boundary is $\;-\frac{w_0}{w_1}$ and the offset is $\;-\frac{b}{w_1}$
    \item figure \ref{821}: $w_0$ only affects the slope
    \item figure \ref{822}: varying $w_1$ also results in different offsets
    \item figure \ref{823}: increasing $b$ leads to a negative offset, scaled by $w_1$
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{821.png}
    \caption{}
	\label{821}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{822.png}
    \caption{}
	\label{822}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{823.png}
    \caption{}
	\label{823}
\end{figure}

\item
A linear classifier divides the input space into two half-spaces. The \textit{shattering coefficient} $s_{halfspaces}(n)$ then is the largest number of subsets that can be formed by intersecting \textit{any} one input set $X$ of $n$ points using half-spaces. The highest possible shattering coefficient is $2^{n}$.

$s_{H}(n) = \underset{X \subseteq \Omega, |X| = n}{max}\ |\{X \cap H_i \mid H_i \in H\}|$

Similarly, the VC dimension can be interpreted as the largest number of data points, that can be separated in all possible ways by the functions contained in the space H (Rychetsky (2001), p.15-16).
It provides a measure of the capacity of the separation capabilities of the separator function class H. If we can find n points, that can be shattered by H (that is, H can separate all possible binary labelings of the points), then VC(H) = n. It is sufficient, that some such example of n points exists (fixed points, but H must shatter them for all possible labelings). It factors into the complexity term and thus into the upper bound of the generalization error (see formula 2.22 in the script). It is itself upper bounded by $min(\frac{R^2}{d_w^2}, N) + 1$, where N is the dimensionality of the input space, R is the radius of the smallest sphere containing the input vectors, and $d_w$ is the margin of the decision boundary.

For a linear classifier in 2 input dimensions, the VC-dimension is 2 (it cannot cannot shatter 3 points, which lie on a line). 

Small VC-dimensions are often applicable for real life data, since close points will have same labels, so separator classes with low VC-dimensions (low complexity models) might actually be preferrable.
Correct linear classification is possible, if all classes can be bounded by non-overlapping convex domains.

\end{enumerate}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.3 The primal SRM problem}
\begin{enumerate}[a)]
\item
The margin is the lowest distance between the decision boundary and the closest datapoints. The margin of a canonical hyperplane is set to be $d = \frac{1}{|w|}$ by choosing $|w|$ appropriately.
A low margin means higher model complexity $C$, which means we might get a worse upper bound for the generalization error $E^G < E^T + C$, even if the separate the training set perfectly (overfitting).
A high margin reduces model complexity, but increases the training error (underfitting).
\item
\input{83b}
\item
Primal Problem:

minimize $f_{0}(x)$ subject to $f_{k}(x) \leq 0$, $k=1,...,m$, which in our case is:

minimize $\frac{1}{2}|w|^2$ subject to $-y_T^{(\alpha)}((w^Tx^{(\alpha)}+b)-1) \leq 0$
with variables: $w, b$

This means we wish to find the largest margin $\frac{1}{|w|}$, that seperates all training datapoints:

$y_T^{(\alpha)}(w^Tx^{(\alpha)} + b) \geq 1 \quad \forall \alpha$

holds for
$y_T^{(\alpha)} \in \{-1, 1\}, \quad \forall \alpha$ and

$ w^Tx^{(\alpha)} + b \geq y_T^{(\alpha)} \quad \forall y_T^{(\alpha)} = 1$,

$ w^Tx^{(\alpha)} + b \leq y_T^{(\alpha)} \quad \forall y_T^{(\alpha)} = -1$

The constraint term at the top is just a concise rewrite of these 2 inequalities.

Since the objective function $\frac{1}{2}|w|^2$ is convex, it only has one optimal solution. And since this is a standard formulation, we can use solvers to solve it.
%The solution should classify all training points perfectly ($E^T = 0$), while minimizing the complexity $C$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.4 The dual problem for the Support Vector Machine}
\begin{enumerate}[a)]
\item
If we have a constrained optimization problem, where we maximize $f(x)$ s.t. $g(x)=0$, we can observe that in the maximum point $\nabla g$ and $\nabla f$ are parallel. This means we can introduce {\em Lagrangian multipliers} $\lambda$ with $\nabla g = \lambda \nabla f$ or $L = f(x) + \lambda g(x), \nabla L = 0$ and solve the resulting equations to find the maximum point.

 Furthermore, if the Lagrangian has a saddle point, the optimal solution to the primal and dual problem is the same, so we can solve the primal by solving the dual problem (script equation 2.29, p.59).

% $\lambda=0$ will be chosen, if $y_T^{(\alpha)}(w^Tx^{(\alpha)} + b) > 1$, i.e. it would have been subtracted from $\frac{1}{2}|w|^2$ in the objective function, which is undesirable for the solver.

% The only case, where $\lambda\geq 0$ would be chosen, is if the corresponding $y_T^{(\alpha)}(w^Tx^{(\alpha)} + b) = 1$, so $y_T^{(\alpha)}(w^Tx^{(\alpha)} + b) - 1 = 0$, i.e. it doesn't lower the value of the objective function.

Only all the closest vectors with the same distance to the decision boundary will have corresponding $\lambda^{(\alpha)} \not= 0$, since for these vectors the constraints will be satisfied with equality, because we're maximizing the margin $\frac{2}{|w|}$.

\item
(script ("lecture notes") equations 2.30 - 2.33ff, p.59-60)

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8.5 Kernel Construction}
\begin{enumerate}[a)]
\item kernel matrix is symmetric:
\begin{eqnarray}
K(x,x')\;=\;\phi(x)^T\,\phi(x')\;=\;\phi(x')^T\,\phi(x)\;=\;K(x',x)
\end{eqnarray}
\item kernel matrix is positive semidefinite:
\begin{eqnarray}
a^T K a\;=\;\sum^P_{i,j=0} a_i a_j K_{ij}\;=\;\sum^P_{i,j=0}a_i a_j \langle \phi(x_i), \phi(x_j) \rangle \\
= \langle\; \sum^P_{i=0}a_i \phi(x_i) \;,\; \sum^P_{j=0} a_j \phi(x_j)\;\rangle \;=\; || \sum_{i=0}^P a_i \phi(x_i) ||^2 \;\geq\;0
\end{eqnarray}
\item sum of two kernels is also a valid kernel:
\begin{itemize}
\item the sum of two symmetric matrices is (obviously) also symmetric and the sum of two positive semidefinite matrices is positive semidefinte as well:
\end{itemize}
\begin{eqnarray}
a^T (K_1\;+\;K_2) a\;=\;a^T K_1 \,a\;+\;a^T K_2 \,a\;=\;c_1\;+\;c_2\;\geq\;0
\end{eqnarray}
\begin{itemize}
\item this makes the sum of those matrices also a valid kernel, as Mercer's theorem states (Haykin - \textit{"Neural Networks - A Comprehensive Foundation"},$\;2^{\mbox{\tiny{nd}}}$ Ed., p. 331)
\end{itemize}
\end{enumerate}

\end{document}
