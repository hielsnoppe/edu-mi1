\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, top=1in, bottom=1.25in, left=0.75in, right=0.75in]{geometry}

\begin{document}
\pagestyle{fancy}
\fancyhead[R]{Niels Hoppe xxxxxx, Robert Sch√ºle xxxxxx, Christoph Ende 331655}

\section{Math Primer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Distributions and expected values}

\begin{enumerate}[a)]

\item

Requirement for $p(x)$ to be probability density function (PDF):

\begin{displaymath}
\int_{-\infty}^{\infty} p(x) \, dx = 1
\end{displaymath}

Calculate antiderivative $P(x)$:

\begin{displaymath}
P(x) = \begin{cases}
-c \cdot cos(x) + const.   & x \in [0, \pi]\\
const.                      & else
\end{cases}
\end{displaymath}

Choose $const.$ to be $0$.

\begin{eqnarray*}
1   &   = & \int_{-\infty}^\infty p(x) \, dx\\
    &   = & \int_0^\pi p(x) \, dx\\
    &   = & \Big[ P(x) \Big]_0^\pi\\
    &   = & -c \cdot cos(\pi) + c \cdot cos(0)\\
    &   = & 2c\\
0.5 &   = & c
\end{eqnarray*}

\item

Expected value of PDF $p(x)$:

\begin{displaymath}
\left\langle X \right\rangle_p = \int_{-\infty}^\infty x \cdot p(x) \, dx
\end{displaymath}

Calculate antiderivative $Q(x)$ of $x \cdot p(x)$:

\begin{displaymath}
Q(x) = \begin{cases}
0.5 \, (sin(x) - x \cdot cos(x))    & x \in [0, \pi]\\
const.                              & else
\end{cases}
\end{displaymath}

Choose $const.$ to be $0$.

\begin{eqnarray*}
\left\langle X \right\rangle_p  &   = & \int_{-\infty}^\infty x \cdot p(x) \, dx\\
                                &   = & \int_0^\pi x \cdot p(x) \, dx\\
                                &   = & \Big[ Q(x) \Big]_0^\pi\\
                                &   = & 0.5 \, (sin(\pi) - \pi \cdot cos(\pi)) - 0.5 \, (sin(0) - 0 \cdot cos(0))\\
                                &   = & \frac{\pi}{2}
\end{eqnarray*}

\item

% TODO: This might not be correct.

Calculate antiderivative $R(x)$ of $\left( x - \left\langle X \right\rangle_p \right)^2 \cdot p(x)$:

\begin{eqnarray*}
\left( x - \left\langle X \right\rangle_p \right)^2 \cdot p(x)
&   = & \left( x - \frac{\pi}{2} \right)^2 \cdot \frac{1}{2} \cdot sin(x)\\
&   = & \left( x^2 - \pi x + \frac{\pi^2}{4} \right) \cdot \frac{1}{2} \cdot sin(x)
\end{eqnarray*}

\begin{displaymath}
R(x) = \begin{cases}
\frac{1}{2} \left( -x^2 + \pi x - \frac{1}{4} \, \pi^2 - 2 \right)
\cdot cos(x) + \left( x - \frac{1}{2} \, \pi \right) \cdot sin(x) + const.      & x \in [0, \pi]\\
const.                                                                          & else
\end{cases}
\end{displaymath}

Choose $const.$ to be $0$.

\begin{eqnarray*}
\left\langle X^2 \right\rangle_p - \left\langle X \right\rangle_p^2
&   = & \int_{-\infty}^\infty \left( x - \left\langle X \right\rangle_p \right)^2 \cdot p(x) \, dx\\
&   = & \int_0^\pi \left( x - \frac{\pi}{2} \right)^2 \cdot \frac{1}{2} \cdot sin(x) \, dx\\
&   = & \Big[ R(x) \Big]_0^\pi\\
&   = & \frac{1}{2} \left( -\pi^2 + \pi^2 - \frac{1}{4} \pi^2 - 2 \right) \cdot cos(\pi)
        + \left( \pi - \frac{1}{2} \pi \right) \cdot sin(\pi)
        - \frac{1}{2} (-2) \cdot cos(0) - 0\\
&   = & \frac{1}{8} \pi^2 + 1 + 1\\
&   = & 2 + \frac{1}{8} \pi^2\\
& \approx & 3.2337
\end{eqnarray*}

\end{enumerate}

\paragraph{c)}
\begin{itemize}
\item calculation of the variance $\;\langle X^2\rangle_p\;-\;\langle X\rangle_p^2$:
\end{itemize}

\begin{eqnarray}
\langle X^2\rangle_p\;-\;\langle X\rangle_p^2\;&=&\;\int_{-\infty}^{\infty} (x-\langle X \rangle_p)^2 \;p(x)dx \\
&=& \frac{1}{2}\int_{0}^{\pi} (x- \frac{\pi}{2})^2\;sin(x)dx \\
&=& \frac{1}{2} [\;\int_{0}^{\pi}x^2\;sin(x)\;dx\; - \int_{0}^{\pi}\pi\,x\,sin(x)\,dx\; + \int_{0}^{\pi}\frac{\pi^2}{4}\;sin(x)\,dx\;] \\
&=& \frac{1}{2} [\;\int_{0}^{\pi}x^2\;sin(x)\;dx\; - \pi^2 + \int_{0}^{\pi}\frac{\pi^2}{4}\;sin(x)\,dx\;] \\
&=& \frac{1}{2} [\;\int_{0}^{\pi}x^2\;sin(x)\;dx\; - \pi^2 - \frac{\pi^2}{4}\;cos(x)\,\Big{\vert}_0^\pi\;] \\
&=& \frac{1}{2} [\;\; -x^2\,cos(x)\,\Big{\vert}_0^\pi\,+\,\int_0^\pi x\,cos(x)\,dx \; - \pi^2 + \frac{\pi^2}{2}] \\
&=& \frac{1}{2} [\;\; \pi^2\,+\,2\int_0^\pi x\,cos(x)\,dx \; - \frac{\pi^2}{2}] \\
&=& \frac{1}{2} [\;2x\,sin(x)\Big{\vert}_0^\pi\;-\;2\int_0^\pi\,sin(x)\,dx \; + \frac{\pi^2}{2}] \\
&=& \frac{1}{2} [\;-2\int_0^\pi\,sin(x)\,dx \; + \frac{\pi^2}{2}] \\
&=& \frac{1}{2} [\;-4\;+ \frac{\pi^2}{2}] \\
&=& \frac{\pi^2}{4}\,-\,2\\
&\approx&\;0.4674
\end{eqnarray}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Marginal densities}

\begin{enumerate}[a)]
\item to calculate the marginal densities, we need to solve the following integrals:
\end{enumerate}
  \begin{eqnarray}
p_x(x)&\;=\;& \int_{-\infty}^{\infty} \,p_{x,y}(x,\tilde{y})\;d\tilde{y} \\
p_y(y)&\;=\;& \int_{-\infty}^{\infty} \,p_{x,y}(\tilde{x},y)\;d\tilde{x}
  \end{eqnarray}

  \begin{eqnarray}
p_x(x)&\;=\;& \frac{3}{7}\int_{0}^{1} \,2x-x^2+\tilde{y}(2-x)\;d\tilde{y} \\
&\;=\;& \frac{3}{7}\,(2x-x^2)\tilde{y}\,+\,\tilde{y}^2\frac{3(2-x)}{14} \Big{\vert}_0^1\\
&\;=\;& \frac{3}{7}\,(2x-x^2)+\frac{6-3x}{14}\\
&\;=\;& -\frac{3}{7}x^2 + \frac{9}{14}x + \frac{6}{14}
  \end{eqnarray}

  \begin{eqnarray}
p_y(y)&\;=\;& \frac{3}{7}\int_{0}^{2} \,2\tilde{x}+2y-\tilde{x}^2-\tilde{x}y\;d\tilde{x} \\
&\;=\;& \frac{3}{7}\Big{(}\;\tilde{x}^2+2\tilde{x}y-\frac{1}{3}\tilde{x}^3-\frac{1}{2}\tilde{x}^2y\;\Big{) \vert}_0^2 \\
&\;=\;& \frac{3}{7}\Big{(}\;2y+\frac{4}{3}\;\Big{)}\\
&\;=\;& \frac{6}{7}y\;+\;\frac{12}{21}
  \end{eqnarray}


\begin{enumerate}[b)]

\item if $x$ and $y$ are statistical independent the probability density function needs to be writable as $p_{x,y}(x,y)\,=\, p(x)\,p(y)$
\begin{itemize}
\item the roots of $x$ need to be independent of $y$
\begin{equation}
p_{x,y}(x,y)\;=\; \frac{3}{7}(2-x)(x+y)\;=\;-\frac{3}{7}(x-2)(x+y)\;=\;-\frac{3}{7}(x-x_{0,1})(x-x_{0,2})
\end{equation}
\item since the roots of x, $x_{0,1}=2$ and $x_{0,2}=-y$, depend on y, the variables are not independent
\end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Taylor expansion}

General form of the taylor series:

\begin{displaymath}
\sum_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n
\end{displaymath}

Calculate derivatives up to $n=3$:

\begin{eqnarray*}
f'(x)   &   = & \frac{1}{2 \, \sqrt{x + 1}}\\
f''(x)  &   = & - \frac{1}{4 \, (x + 1)^\frac{3}{2}}\\
f'''(x) &   = & \frac{3}{8 \, (x + 1)^\frac{5}{2}}\\
\sum_{n=0}^3 \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n
        &   = & 1 + \frac{x}{2} - \frac{x^2}{8} + \frac{x^3}{16}
\end{eqnarray*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Determinant of a matrix}

\begin{eqnarray*}
A   &   = & \begin{pmatrix}
  5 &   8 &  16\\
  4 &   1 &   8\\
 -4 &  -4 & -11
\end{pmatrix}\\
det(A)  &   = & a_{11} a_{22} a_{33} + a_{12} a_{23} + a_{31} + a_{13} a_{21} a_{32}
            - a_{31} a_{22} a_{13} - a_{32} a_{23} a_{11} - a_{33} a_{21} a_{12}\\
        &   = & 5 \cdot 1 \cdot (-11) + 8 \cdot 8 \cdot (-4) + 16 \cdot 4 \cdot (-4)
            - (-4) \cdot 1 \cdot 16 - (-4) \cdot 8 \cdot 5 - (-11) \cdot 4 \cdot 8\\
        &   = & -55 - 256 - 256 + 64 + 160 + 352\\
        &   = & 9\\
tr(A)   &   = & a_{11} + a_{22} + a_{33}\\
        &   = & 5 + 1 - 11\\
        &   = & -5
\end{eqnarray*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Critical points}

\begin{enumerate}[a)]

\item Check if $a=(0,0)$ is a critical point for $f$ and $g$.

Calculate Jabobimatrix (first derivatives):

\begin{align*}
	Jf &= (2x, 2y) \\
	Jg &= (2x, -2y)
\end{align*}

A critical point requires all first derivatives to be zero:

\begin{align*}
	2x = 0 \Leftrightarrow x = 0 \\
	2y = 0 \Leftrightarrow y = 0
\end{align*}

This means $a = (0,0)$ is a critical point for $f$. Now we do the same for $g$:

\begin{align*}
	2x = 0 \Leftrightarrow x = 0 \\
	-2y = 0 \Leftrightarrow y = 0
\end{align*}

We get that $a$ is a critical point for $g$, too.

\item Check if $a$ is an extremum.

Calculate Hessian matrix:

\begin{align*}
	Hf &= \left(\begin{array}{ccc}
		2 & 0 \\
		0 & 2
		\end{array} \right) \\
	Hg &= \left(\begin{array}{ccc}
		2 & 0 \\
		0 & -2
		\end{array} \right) \\
\end{align*}

Obviously, $det(Hf(a)) \neq 0$ and $det(Hg(a)) \neq 0$.

Now we calculate the Eigenvalues:
\begin{align*}
	det(Hf(a) - \lambda I) = 0
	\Leftrightarrow det \left(\begin{array}{ccc} 2-\lambda & 0 \\ 0 & 2-\lambda \end{array} \right) = 0
	\Leftrightarrow (2-\lambda)^2 = 0
	\Leftrightarrow \lambda = 2
\end{align*}
All Eigenvalues are positive, so $Hf(a)$ is positive definite, which means $a$ is a minimum of $f$.

For $Hg(a)$ we observe positive and negative Eigenvalues:
\begin{align*}
	det(Hg(a) - \lambda I) = 0
	\Leftrightarrow det \left(\begin{array}{ccc} 2-\lambda & 0 \\ 0 & -2-\lambda \end{array} \right) = 0
	\Leftrightarrow (2-\lambda)(-2-\lambda) = 0
	\Leftrightarrow \lambda \in \{2,-2\}
\end{align*}
This means Hg(a) is neither positive nor negative definite, so $a$ is not an extremum of $g$.

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayes rule}
From the description we get
\begin{align*}
	P(D) &= 0.01 \\
	P(\bar{D}) &= 0.99 \\
	P(+|D) &= 0.95 \\
	P(-|D) &= 0.05 \\
	P(+|\bar{D}) &= 0.001 \\
	P(-|\bar{D}) &= 0.999
\end{align*}

from which we can calculate the remaining probabilities using Bayes rule:

\begin{align*}
P(D|+) &= \frac{P(+|D) P(D)}{P(+|D)P(D) + P(+|\bar{D})P(\bar{D})}
	= \frac{0.95 \cdot 0.01}{0.95 \cdot 0.01 + 0.001 \cdot 0.99}
	\approx 0.9056 \\
P(\bar{D}|+) &= 1 - P(D|+) \approx 0.0944 \\
P(\bar{D}|-) &= \frac{P(-|\bar{D})P(\bar{D})}{P(-|\bar{D})P(\bar{D}) + P(-|D)P(D)}
	= \frac{0.999 \cdot 0.99}{0.999 \cdot 0.99 + 0.05 \cdot 0.01}
	\approx 0.9995 \\
P(D|-) &= 1 - P(\bar{D}|-) \approx 0.0005
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning paradigms}

\begin{enumerate}[a)]

\item \begin{itemize}

\item In supervised learning we provide control labels to the machine during training, like a teacher would do. We can measure the performance by calculating the Error between targets and actual results.

\item In unsupervised learning the machine has to learn without outside help, so its only inputs are the observations. This yields us the opportunity to discover hidden structures in our data.

\item In reinforcement learning, additionally to providing the machine with observations like always, we give feedback to its classifications of our data (reinforcement signal, ranging anywhere between "good" and "bad"). We can measure the performance by calculating the cumulative reward.

\end{itemize}

\item \begin{itemize}
\item To teach a (robot?) dog to catch a ball

Reinforcement learning seems appropriate. As observations we would provide the trajectory of the ball. As feedback to the dog's movements we would provide the distance between dog and ball (lower is better), whether the dog has lifted the ball, and once he has, the distance between trainer and dog (lower is better).

\item To read hand written addresses from letters

Here we would employ reinforcement learning with a training phase, as we cannot train each persons handwriting to the machine right from the start. As observations one image per word seems appropriate. As feedback one could provide a combination of count of mistakes for some proof-read samples, and for each letter wether it could actually be delivered or not.

If the user set was limited, providing huge amounts of handwritten address samples (again one image per word) with their manually digitized versions as control labels might suffice.

\item identify groups of users with the same taste of music

Here we would employ unsupervised learning to discover clusters in our data. As observations we could provide (pre-calculated) metadata of each user's music-collection, like user-id, bpm per track, genre-tags etc. .
\end{itemize}

\end{enumerate}

\end{document}
