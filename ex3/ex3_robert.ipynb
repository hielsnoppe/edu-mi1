{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5503  0.9206  0.5359  0.6081  0.0202  0.8545  0.2357  0.4847  0.3996\n",
      "  0.1957]\n",
      "[-0.5894 -0.2507 -0.0468 -0.3402  0.2857 -1.0683  0.8605 -0.0801  0.6837\n",
      "  1.185 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "# import scipy.linalg as la\n",
    "%matplotlib inline\n",
    "\n",
    "data = np.loadtxt(open(\"RegressionData.txt\",\"rb\"), delimiter=\" \", skiprows=0)\n",
    "# print data\n",
    "\n",
    "X = data[:,0]  # random numbers x_n drawn from uniform distribution over [0,1]\n",
    "print X\n",
    "Y = data[:,1]  # labels/target values t_n for x_n, generated using 2*pi*x_n, gaussian noise with standard deviation sigma=0.25\n",
    "print Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.         -1.        ]\n",
      " [ 0.5503      0.19311316  0.16833644]\n",
      " [ 0.9206      0.69444663 -0.20397982]\n",
      " [ 0.5359     -0.71719249  0.54402566]\n",
      " [ 0.6081      0.         -0.15778227]\n",
      " [ 0.0202      0.          0.52256829]\n",
      " [ 0.8545      0.         -0.64352454]\n",
      " [ 0.2357      0.         -0.32804654]\n",
      " [ 0.4847      0.         -0.07573577]\n",
      " [ 0.3996      0.          0.83123771]\n",
      " [ 0.1957      0.         -0.08519495]] activations S\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.58233134 -0.4416484   1.        ]\n",
      " [ 0.13213969  0.02561372  1.        ]\n",
      " [-0.32902132 -0.42620419  1.        ]\n",
      " [ 0.          0.04266189  1.        ]\n",
      " [ 0.          0.06881398  1.        ]\n",
      " [ 0.         -0.68065353  1.        ]\n",
      " [ 0.         -0.05264848  1.        ]\n",
      " [ 0.          0.32292986  1.        ]\n",
      " [ 0.          0.71617221  1.        ]\n",
      " [ 0.          0.6170374   1.        ]] local error D\n",
      "[ 0.          0.75773644  0.04672018  0.59082566  0.18241773  0.23686829\n",
      "  0.42477546 -1.18854654  0.00436423  0.14753771 -1.27019495] dedy\n",
      "(11, 11) dydw[0].shape\n",
      "(11, 11) dedw[i].shape\n",
      "[[  0.00000000e+00   8.00916582e-03   8.26123657e-04   6.08152798e-03\n",
      "    2.13064776e-03   9.19026205e-05   6.97173846e-03  -5.38078175e-03\n",
      "    4.06303301e-05   1.13239549e-03  -4.77453200e-03]\n",
      " [  0.00000000e+00   1.46328876e-01   3.24446698e-02  -4.23735729e-01\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00]] gradient eta*G\n",
      "(2, 11) gradient shape\n",
      "(11, 11)\n",
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "def init(X, layers=[10, 3, 10]):\n",
    "    rows = len(X)+1  # include bias node\n",
    "    columns = len(layers)\n",
    "\n",
    "    # activations, 3 columns: bias + input layer, 1 hidden layer, output layer\n",
    "    S = np.zeros((rows, columns))\n",
    "    \n",
    "    # bias\n",
    "    S[0,:] = -1.\n",
    "\n",
    "    # connection weights W[from-layer][from-index, to-index]\n",
    "    W = [np.random.uniform(-0.5, 0.5, rows*rows).reshape(rows, rows) for i in range(columns-1)]\n",
    "#     W = [np.ones((rows, rows)) for i in range(columns-1)]\n",
    "    for w in range(len(W)):\n",
    "        left = layers[w]  # max from-index\n",
    "        right = layers[w+1]  # max to-index\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(rows):\n",
    "                # only allow weights between nodes that exist in the model\n",
    "                # may not come from too high index\n",
    "                # may not go to too high index\n",
    "                # may not go to bias node\n",
    "                if i > left or j > right or j == 0:\n",
    "                    W[w][i,j] = 0.\n",
    "#         print W[w], \"W[\"+str(w)+\"]\"\n",
    "    \n",
    "    # transfer fct\n",
    "    f = np.tanh\n",
    "#     f = lambda x: x\n",
    "    # 1st derivative of transfer fct\n",
    "    fd = lambda h : (1./np.cosh(h))**2\n",
    "\n",
    "    return S, W, f, fd\n",
    "\n",
    "# calculate activations\n",
    "def forward(S, W, X, f):\n",
    "    rows, columns = S.shape\n",
    "    \n",
    "    # init\n",
    "    H = np.zeros(S.shape)\n",
    "    S[1:,0] = X.T\n",
    "    H[1:,0] = X.T\n",
    "    \n",
    "    # propagation\n",
    "    for v in range(1, columns):\n",
    "        S[1:,v] = H[1:,v] = W[v-1].T.dot(S[:,v-1])[1:]\n",
    "        if v < columns-1:\n",
    "            S[1:,v] = f(H[1:,v])\n",
    "    \n",
    "    return H, S\n",
    "\n",
    "# calculate output error as quadratic error\n",
    "def output_error(Yt, Yx):  # target labels, computed labels\n",
    "#     print Yt, len(Yt)\n",
    "#     print Yx, len(Yx)\n",
    "    return ((Yt-Yx)**2)/2.\n",
    "\n",
    "# calculate local error for each training point\n",
    "def backward(S, H, W, Eo, fd):\n",
    "    rows, columns = S.shape\n",
    "    \n",
    "    # init\n",
    "    D = np.zeros((rows, columns))\n",
    "    D[1:,-1] = 1  # =1 if f = identity\n",
    "    \n",
    "    # propagate\n",
    "    for v in range(columns-1)[::-1]:\n",
    "        D[:,v] = fd(H[:,v]) * (D[:,v+1].T.dot(W[v]))\n",
    "    \n",
    "    return D\n",
    "\n",
    "def gradient(S, D, Yt, Yx):\n",
    "    rows, columns = S.shape\n",
    "    # 1st factor as per script p. 17 (1.9)\n",
    "    dedy = np.insert(Yx - Yt, 0, 0)  # row vector\n",
    "    print dedy, \"dedy\"\n",
    "    \n",
    "    # 2nd factor as per script p. 18 (1.11)\n",
    "    dydw = [D[:,i+1,np.newaxis] * S[np.newaxis,:,i] for i in range(columns-1)]\n",
    "    # dydw should now be a quadratic matrix indexed as dydw[from,to]\n",
    "#     print dydw, \"dydw\"\n",
    "    print dydw[0].shape, \"dydw[0].shape\"\n",
    "    \n",
    "    # script p. 17 (1.8)\n",
    "    dedw = [dydw[i] * dedy[np.newaxis,:] for i in range(len(dydw))]\n",
    "    print dedw[i].shape, \"dedw[i].shape\"\n",
    "    \n",
    "    # script p. 17 (1.7)\n",
    "    N = len(Yt)\n",
    "    G = (1./N) * (np.sum(dedw, 1))\n",
    "    \n",
    "    return G\n",
    "\n",
    "def weight_update(W, G, eta):\n",
    "    print W[0].shape\n",
    "    print G.shape\n",
    "    \n",
    "    for w in range(len(W)):\n",
    "        W[w] = W[w] - eta*G[w]\n",
    "    \n",
    "    return W\n",
    "\n",
    "def ex31(X, Yt, S, W, f, fd):\n",
    "#     rows, columns = S.shape\n",
    "#     W[0][1,1] = 1\n",
    "#     W[1][1,3] = 1\n",
    "    H, S = forward(S, W, X, f)\n",
    "#     print H, \"total input of transfer fct: H\"\n",
    "    print S, \"activations S\"\n",
    "    \n",
    "    Eo = output_error(Yt, S[1:,-1])\n",
    "#     print Eo, \"output error\"\n",
    "    \n",
    "    D = backward(S, H, W, Eo, fd)\n",
    "    print D, \"local error D\"\n",
    "\n",
    "    G = gradient(S, D, Yt, S[1:,-1])\n",
    "    print G, \"gradient eta*G\"\n",
    "    print G.shape, \"gradient shape\"\n",
    "    \n",
    "    eta = .5\n",
    "    W = weight_update(W, G, eta)\n",
    "\n",
    "# cache variables\n",
    "_S, _W, _f, _fd = init(X)\n",
    "ex31(X, Y, _S, _W, _f, _fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
