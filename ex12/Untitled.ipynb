{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2\n",
    "\n",
    "## a)\n",
    "- Bayesian learning belongs to the class of generative models (opposite: discriminative models), where the output prediction $ P(y \\vert x) $ is calculated from the marginal pdf $ P(x\\vert y) $ and the conditional pdf $ P(x\\vert y) $ by applying Bayes' rule\n",
    "- For Bayesian learning, the inputs $x^{(1)},...,x^{(n)}$ are used to find reasonable model parameters $\\theta$. Before using training data to perform the learning algorithm, a prior distribution $P(\\theta)$ needs to be defined, by using initial beliefs about the model parameters. When working with MLPs, the model parameters are represented by the weights and biases of the single perceptrons.\n",
    "- To capture the impact of observations on the model parameters, a likelihood function is introduced: \n",
    "$$ L( \\theta)=L( \\theta | x^{(1)},...,x^{(n)}) \\propto P(x^{(1)},...,x^{(n)}| \\theta) $$\n",
    "- After the observation of the input, the pdf $P(\\theta | x^{(1)},...,x^{(n)})$ will be updated, now called posterior function by using \n",
    "- The predictive distribution is the Bayesian inference and allows us to predict the value of an unknown input $x^{(n+1)}$ given privious inputs by eleminating the model parameters of known distributions: $$ P(x^{(n+1)}|x^{(1)},...,x^{(n)}) \\;=\\; \\int P(x^{(n+1)}| \\theta) P( \\theta | x^{(1)},...,x^{(n)}) d \\theta $$\n",
    "\n",
    "\n",
    "\n",
    "## b)\n",
    "- The concept, that model complexity should be limited to avoid an overfitted model doesn't apply to Bayesian learning in a strong matter. This leads to the rule that complexity should only be limited by computational restrictions.\n",
    "- Bayesian learning is not a method to find scientific explanations or estimate real parameter (nonparametric, since used parameters like weights have no physical meaning for the problem itself) and it's complexity can not simply be defined by the amount of parameters what makes it very different from other approaches.\n",
    "- When training the model, the weight decay must be well adjusted to avoid both overfitting and underfitting. This is difficult because there is no obvious relationship between the weights and the actual problem.\n",
    "- Underfitting / overfitting is difficult to control, caused by several reasons. E.g. the prior distribution needs to be choosen arbitrary and in many cases, choosing them by random delivers resonable results. The same applies to the weight penalty. These problems can partially be reduced by using cross validation. It's also common to work with rules of thumb.\n",
    "- MLPs are hierarchical what gives them one more degree of freedom and makes them more flexible. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## c)\n",
    "\n",
    "- SRM is a deterministic approach for finding a model with a good balance between complexity and the success on test data. Models of the same function class but with different complexities ( possibly represented by VC-dimension or the degree of used functions) are sortet according to their complexity, which might cause overfitting when too large and the empirical (training) error, which tends towards zero for overfitted models. \n",
    "- Each model class is tested for the best parameter set (empirical risk minimisation) in order to find the best compromise of empirical error and complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
